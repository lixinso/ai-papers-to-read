# Best AI Papers to Read
### [That are publicly available]

## Papers

| Title | Link | Author | Date | Highlight | 
|-------|------|--------|------|------|
| The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation | https://ai.meta.com/blog/llama-4-multimodal-intelligence/ | - | - |  mixture-of-experts (MoE) architecture | 
| Why Do Multi-Agent LLM Systems Fail? | https://github.com/multi-agent-systems-failure-taxonomy/MASFT <br> https://huggingface.co/papers/2503.13657 | - | - | - | 
| MoBA: Mixture of Block Attention for Long-Context LLMs | [arXiv](https://arxiv.org/abs/2502.13189) | - | - | - | 
| Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention | [arXiv](https://arxiv.org/abs/2502.11089) | - | - | - | 
| Foundations of Large Language Models | [arXiv](https://arxiv.org/abs/2501.09223) | - | - | - | 
| DeepSeek-V3 Technical Report | [GitHub](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf) | - | - | - | 
| Adaptive In-Conversion Team Building For Language Model Agent | [arXiv](https://arxiv.org/pdf/2405.19425) | - | - | - | 
| Agent AI Towards a Holistic Intelligence | [Microsoft](https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AgentAI_p.pdf?utm_source=chatgpt.com) | - | - | - | 
| Large Language Model-Brained GUI Agents: A Survey | [arXiv](https://arxiv.org/pdf/2411.18279) | - | - | - | 
| MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering | [arXiv](https://arxiv.org/abs/2410.07095) | OpenAI | 24 Oct 2024 | - | 
| Agent AI: Surveying the Horizons of Multimodal Interaction | [arXiv](https://arxiv.org/abs/2401.03568) | - | - | - | 
| The Llama 3 Herd of Models | [PDF](https://scontent-sea1-1.xx.fbcdn.net/v/t39.2365-6/468347782_9231729823505907_4580471254289036098_n.pdf?_nc_cat=110&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=kMEnijIEZ-gQ7kNvgGkvbgV&_nc_zt=14&_nc_ht=scontent-sea1-1.xx&_nc_gid=AArxIIwO-RKxc0VHEWEXrCy&oh=00_AYBtVpVUKJJ3gf-Ev83Js4tUNmA_eQCHifdaJapFuVCJtA&oe=67508F80), [Meta AI](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/), [Blog](https://ai.meta.com/blog/meta-llama-3-1/), [Repo](./papers/The%20Llama%203%20Herd%20of%20Models/README.md) | - | - | - | 
| Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks | [Microsoft Article](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/), [Publication](https://www.microsoft.com/en-us/research/publication/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/), [GitHub](https://github.com/microsoft/autogen/tree/main/python/packages/autogen-magentic-one) | - | - | - | 
| AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework | [arXiv](https://arxiv.org/abs/2308.08155) | Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, Chi Wang | ArXiv 2023 | - | 
| World Models David Ha, JÃ¼rgen Schmidhuber  | [arXiv](https://arxiv.org/abs/1803.10122?utm_source=chatgpt.com) | - | - | - | 
| DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning  | [arXiv](https://arxiv.org/abs/2501.12948) | - | - | - | 

## Open Source Repos (with code)

| Title | Link | Author | Date |
|-------|------|--------|------|
| OLMo | [link]([https://arxiv.org/abs/2502.13189](https://github.com/allenai/OLMo)) | - | - |
| nanoGPT | [link](https://github.com/karpathy/nanoGPT) | - | - |
| gigaGPT | [link](https://github.com/Cerebras/gigaGPT) | - | - |
| Megatron-LM | [link](https://github.com/NVIDIA/Megatron-LM) | - | - |
| gpt-2 | [link](https://github.com/openai/gpt-2/tree/master) | - | - |
| grok-1 | [link](https://github.com/xai-org/grok-1) | - | - | 
| open-r1 | [link](https://github.com/huggingface/open-r1)  | - | - | 
| EleutherAI gpt-neox | [link](https://github.com/EleutherAI/gpt-neox)  | - | - | 





