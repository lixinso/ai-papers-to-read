# Best AI Papers to Read
### [That are publicly available]

## Papers

| Title | Link | Author | Date | Highlight | 
|-------|------|--------|------|------|
|gpt-oss|https://github.com/openai/gpt-oss|--------|------|------|
|The Ultra-Scale Playbook:Training LLMs on GPU Clusters | https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=high-level_overview |Hugging Face|Feb 19, 2025|------|
|Magentic-UI: Towards Human-in-the-loop Agentic Systems|https://arxiv.org/abs/2507.22358v1|Microsoft Research AI Frontiers|------|------|
| Hierarchical Reasoning Model | https://arxiv.org/html/2506.21734v1 | Guan Wang1, Jin Li1, Yuhao Sun1, Xing Chen1, Changling Liu1, Yue Wu1, Meng Lu1, Sen Song2, Yasin Abbasi Yadkori1, Sapient Intelligence, Singapore |------|------|
|DeepSeek-R1|https://github.com/deepseek-ai/DeepSeek-R1|--------|------|Distillation: Smaller Models Can Be Powerful Too|
|Training language models to follow instructions with human feedback|------|Ouyang et al.,|2022|often called the InstructGPT paper-|
|Textbooks Are All You Need (phi-1)|------|2023 06|Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar|2306.11644 |
|TinyStories: How Small Can Language Models Be and Still Speak Coherent English?|2305.07759 | Ronen Eldan, Yuanzhi Li|2023 05|------|
|Orca: Progressive Learning from Complex Explanation Traces of GPT-4|2306.02707 |Subhabrata Mukherjee, Arindam Mitra, Hamid Palangi, Ahmed H. Awadallah|2023 06|------|
|Small Language Models are the Future of Agentic AI|https://arxiv.org/pdf/2506.02153|--------|------| LLM-to-SLM Agent Conversion|
|GPT-4 Technical Report|https://arxiv.org/abs/2303.08774?utm_source=chatgpt.com|--------|------|GPT-4 report includes 26 expert exams; illustrates real-world barometers. State-of-the-art multimodal. Illustrates current limits and evaluation style of frontier models |
|A Survey on Large Language Models with some Insights on their Capabilities and Limitations| https://arxiv.org/pdf/2501.04040 |--------|------|------|
|Potemkin Understanding in Large Language Models | https://arxiv.org/abs/2506.21521?utm_source=chatgpt.com | Marina Mancoridis, Bec Weeks, Keyon Vafa, Sendhil Mullainathan | 26 Jun 2025 |You can not create AGI with LLM|
|Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI |https://arxiv.org/abs/2505.19443|--------|------|------|
|“V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning.”| https://arxiv.org/abs/2506.09985 | Mido Assran, Adrien Bardes, David Fan, et al. (29 co-authors, including Yann LeCun) | 11 June 2025| https://github.com/facebookresearch/vjepa2 |
|Self-Evolving Curriculum for LLM Reasoning|https://arxiv.org/pdf/2505.14970|Yoshua Bengio, et.al|May 2025|------|
| Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems | https://arxiv.org/abs/2504.01990 | Bang Liu,, et al | Mar 2025 | [Awesome-Foundation-Agents. Great paper list](https://github.com/FoundationAgents/awesome-foundation-agents) | 
| Foundations of Large Language Models | [(arXiv: 2501.09223 v1 [cs.CL] 16 Jan 2025)](https://arxiv.org/abs/2501.09223)  | Tong Xiao and Jingbo Zhu, NLP Lab, Northeastern University & NiuTrans Research | January 17, 2025 | - |
| Intuitive physics understanding emerges from self-supervised pretraining on natural videos | https://arxiv.org/pdf/2502.11831 | Yann LeCun | - |  V-JEPA --a self-supervised video model |
| DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning | https://arxiv.org/abs/2501.12948 | - | - | - |
| DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Model| https://arxiv.org/pdf/2402.03300 | - | - | - | 
| The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation | https://ai.meta.com/blog/llama-4-multimodal-intelligence/ | - | - |  Mixture-of-Experts (MoE) architecture | 
| Why Do Multi-Agent LLM Systems Fail? | https://github.com/multi-agent-systems-failure-taxonomy/MASFT <br> https://huggingface.co/papers/2503.13657 | - | - | - | 
| MoBA: Mixture of Block Attention for Long-Context LLMs | [arXiv](https://arxiv.org/abs/2502.13189) | - | - | - | 
| Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention | [arXiv](https://arxiv.org/abs/2502.11089) | - | - | - | 
| Foundations of Large Language Models | [arXiv](https://arxiv.org/abs/2501.09223) | - | - | - | 
| DeepSeek-V3 Technical Report | [GitHub](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf) | - | - | - | 
| Adaptive In-Conversion Team Building For Language Model Agent | [arXiv](https://arxiv.org/pdf/2405.19425) | - | - | - | 
| Agent AI Towards a Holistic Intelligence | [Microsoft](https://www.microsoft.com/en-us/research/uploads/prod/2024/02/AgentAI_p.pdf?utm_source=chatgpt.com) | - | - | - | 
| Large Language Model-Brained GUI Agents: A Survey | [arXiv](https://arxiv.org/pdf/2411.18279) | - | - | - | 
| MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering | [arXiv](https://arxiv.org/abs/2410.07095) | OpenAI | 24 Oct 2024 | - | 
| Agent AI: Surveying the Horizons of Multimodal Interaction | [arXiv](https://arxiv.org/abs/2401.03568) | - | - | - | 
| The Llama 3 Herd of Models | [PDF](https://scontent-sea1-1.xx.fbcdn.net/v/t39.2365-6/468347782_9231729823505907_4580471254289036098_n.pdf?_nc_cat=110&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=kMEnijIEZ-gQ7kNvgGkvbgV&_nc_zt=14&_nc_ht=scontent-sea1-1.xx&_nc_gid=AArxIIwO-RKxc0VHEWEXrCy&oh=00_AYBtVpVUKJJ3gf-Ev83Js4tUNmA_eQCHifdaJapFuVCJtA&oe=67508F80), [Meta AI](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/), [Blog](https://ai.meta.com/blog/meta-llama-3-1/), [Repo](./papers/The%20Llama%203%20Herd%20of%20Models/README.md) | - | - | - | 
| Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks | [Microsoft Article](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/), [Publication](https://www.microsoft.com/en-us/research/publication/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/), [GitHub](https://github.com/microsoft/autogen/tree/main/python/packages/autogen-magentic-one) | - | - | - | 
| AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework | [arXiv](https://arxiv.org/abs/2308.08155) | Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, Chi Wang | ArXiv 2023 | - | 
| World Models David Ha, Jürgen Schmidhuber  | [arXiv](https://arxiv.org/abs/1803.10122?utm_source=chatgpt.com) | - | - | - | 
| DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning  | [arXiv](https://arxiv.org/abs/2501.12948) | - | - | - | 
| Attention Is All You Need  | https://arxiv.org/abs/1706.03762 | Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin | - | - | 

## Open Source Repos (with code)

| Title | Link | Author | Date |
|-------|------|--------|------|
| Open-Sora | https://github.com/hpcaitech/Open-Sora | - | - |
| NeMo | https://github.com/NVIDIA/NeMo | Nvidia | - |
| OLMo | [link]([https://arxiv.org/abs/2502.13189](https://github.com/allenai/OLMo)) | - | - |
| nanoGPT | [link](https://github.com/karpathy/nanoGPT) | - | - |
| gigaGPT | [link](https://github.com/Cerebras/gigaGPT) | - | - |
| Megatron-LM | [link](https://github.com/NVIDIA/Megatron-LM) | - | - |
| gpt-2 | [link](https://github.com/openai/gpt-2/tree/master) | - | - |
| grok-1 | [link](https://github.com/xai-org/grok-1) | - | - | 
| open-r1 | [link](https://github.com/huggingface/open-r1)  | - | - | 
| EleutherAI gpt-neox | [link](https://github.com/EleutherAI/gpt-neox)  | - | - | 





